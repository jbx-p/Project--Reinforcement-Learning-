{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a174678e",
   "metadata": {},
   "source": [
    "### Project field\n",
    "\n",
    "\n",
    "  - Use Reinforcement Learning with Q-learning to find solutions to this field.\n",
    "  \n",
    "  <img src=\"img/dd.png\" height = 300 width = 300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ef1f0",
   "metadata": {},
   "source": [
    "### Step1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1a45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a873f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize field and set a random start state\n",
    "        \"\"\"\n",
    "        self.states = [\n",
    "            [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [-1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            ]\n",
    "        self.state = (random.randrange(0, len(self.states)), random.randrange(0, len(self.states[0])))\n",
    "    \n",
    "    def done(self):\n",
    "        \"\"\"\n",
    "        Check if it isn't in a neutral state\n",
    "        \"\"\"\n",
    "        if self.states[self.state[0]][self.state[1]] != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        \"\"\"\n",
    "        Return possible actions in state\n",
    "\n",
    "        Action:\n",
    "               0 => left\n",
    "               1 => right\n",
    "               2 => up\n",
    "               3 => down\n",
    "        \"\"\"    \n",
    "        actions = [0, 1, 2, 3]\n",
    "        if self.state[0] == 0:\n",
    "            actions.remove(2)\n",
    "        if self.state[0] == len(self.states) -1:\n",
    "            actions.remove(3)\n",
    "        if self.state[1] == 0:\n",
    "            actions.remove(0)\n",
    "        if self.state[1] == len(self.states[0]) -1:\n",
    "            actions.remove(1)\n",
    "        return actions\n",
    "\n",
    "    def update_next_state(self, action):\n",
    "        \"\"\" \n",
    "        Update state according to action -> Return state and reward\n",
    "        \"\"\"\n",
    "        x, y = self.state\n",
    "\n",
    "        if action == 0:\n",
    "            if y == 0:\n",
    "                return self.state, -10\n",
    "            self.state = x, y - 1\n",
    "        if action == 1:\n",
    "            if y == len(self.states) -1:\n",
    "                return self.state, -10\n",
    "            self.state = x, y + 1\n",
    "        if action == 2:\n",
    "            if x == 0:\n",
    "                return self.state, -10\n",
    "            self.state = x - 1, y\n",
    "        if action == 3:\n",
    "            if self.state == len(self.states) -1:\n",
    "                return self.state, -10\n",
    "            self.state = x + 1, y        \n",
    "        reward = self.states[self.state[0]][self.state[1]]\n",
    "        return self.state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb45ecc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), True, [1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = Field()\n",
    "field.state, field.done(),field.get_possible_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb6ff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 0), True, [1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " field.update_next_state(2)\n",
    " field.state, field.done(), field.get_possible_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530d169",
   "metadata": {},
   "source": [
    "### Step3: Step 3: Train the model\n",
    "\n",
    "  - Create a q-table initialized to all 0\n",
    "  \n",
    "     - Use q_table = np.zeros(...) (insert values for ...)\n",
    "     \n",
    "  - Set alpha = .5, gamma = 0.5, and epsilon = 0.5\n",
    "  \n",
    "  - Create for-loop iterating 10000\n",
    "  \n",
    "     - Create new field\n",
    "     \n",
    "     - While field not done\n",
    "     \n",
    "        - Get possible actions and assign to actions\n",
    "        \n",
    "        - With probability epsilon take a random action, otherwise take the best action\n",
    "        \n",
    "            - HINT: random.uniform(0, 1) < epsilon\n",
    "            \n",
    "            - HINT: Random action: random.choice(actions), and best action: np.argmax(q_table[field.state])\n",
    "            \n",
    "        - Get current state and assign it to cur_x, cur_y\n",
    "        \n",
    "        - Update next state and get it and the reward\n",
    "        \n",
    "        - Update q_table[cur_x, cur_y, action] = (1 - alpha)q_table[cur_x, cur_y, action] + alpha(reward + gamma*np.max(q_table[next_x, next_y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f4a009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = Field()\n",
    "q_table = np.zeros((len(field.states), len(field.states[0]), 4))\n",
    "\n",
    "alpha = .5\n",
    "epsilon = .5\n",
    "gamma = .5\n",
    "\n",
    "for _ in range(10000):\n",
    "    field = Field()\n",
    "    while not field.done():\n",
    "        actions = field.get_possible_actions()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = np.argmax(q_table[field.state])\n",
    "        \n",
    "        cur_x, cur_y = field.state\n",
    "        (next_x, next_y), reward = field.update_next_state(action)\n",
    "        q_table[cur_x, cur_y, action] = (1 - alpha)*q_table[cur_x, cur_y, action] + alpha*(reward + gamma*np.max(q_table[next_x, next_y]))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c872d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [ -1.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.03125 ,   0.      ,   0.03125 ],\n",
       "        [  0.015625,   0.0625  ,   0.      ,   0.0625  ],\n",
       "        [  0.03125 ,   0.125   ,   0.      ,   0.125   ],\n",
       "        [  0.0625  ,   0.25    ,   0.      ,   0.25    ],\n",
       "        [  0.125   ,   0.5     ,   0.      ,   0.5     ],\n",
       "        [  0.25    ,   0.25    ,   0.      ,   1.      ],\n",
       "        [  0.5     ,   0.125   ,   0.      ,   0.5     ],\n",
       "        [  0.25    ,   0.0625  ,   0.      ,   0.25    ],\n",
       "        [  0.125   ,   0.      ,   0.      ,   0.125   ]],\n",
       "\n",
       "       [[  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [ -1.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.0625  ,   0.015625,   0.015625],\n",
       "        [  0.03125 ,   0.125   ,   0.03125 ,   0.03125 ],\n",
       "        [  0.0625  ,   0.25    ,   0.0625  ,   0.0625  ],\n",
       "        [  0.125   ,   0.5     ,   0.125   ,   0.125   ],\n",
       "        [  0.25    ,   1.      ,   0.25    ,   0.25    ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  1.      ,   0.25    ,   0.25    ,   0.25    ],\n",
       "        [  0.5     ,   0.125   ,   0.125   ,   0.125   ],\n",
       "        [  0.25    ,   0.      ,   0.0625  ,   0.0625  ]],\n",
       "\n",
       "       [[ -5.      ,   0.      ,  -1.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.03125 ,   0.03125 ,   0.      ],\n",
       "        [  0.015625,   0.0625  ,   0.0625  ,   0.      ],\n",
       "        [  0.03125 ,   0.125   ,   0.125   ,   0.      ],\n",
       "        [  0.0625  ,   0.25    ,   0.25    ,   0.      ],\n",
       "        [  0.125   ,   0.5     ,   0.5     ,   0.      ],\n",
       "        [  0.25    ,   0.25    ,   1.      ,   0.      ],\n",
       "        [  0.5     ,   0.125   ,   0.5     ,   0.      ],\n",
       "        [  0.25    ,   0.0625  ,   0.25    ,   0.      ],\n",
       "        [  0.125   ,   0.      ,   0.125   ,   0.      ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4482a26",
   "metadata": {},
   "source": [
    "### Step 4: Solve a task\n",
    "\n",
    "  - To see the path make a variable path = np.zeros((3, 11))\n",
    "  \n",
    "  - Create a field Field()\n",
    "  \n",
    "  - To count steps assign steps = 1\n",
    "  \n",
    "  - Assign the start state in the path to np.nan.\n",
    "  \n",
    "  - The we begin: while not solved.\n",
    "  \n",
    "     - Get the action to take\n",
    "     \n",
    "     - Get the next state\n",
    "     \n",
    "     - Update path with steps\n",
    "     \n",
    "     - Increment steps with one\n",
    "     \n",
    "  - see the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07eee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.zeros((3, 11))\n",
    "field = Field()\n",
    "steps = 1\n",
    "path[field.state[0]][field.state[1]] = np.nan\n",
    "\n",
    "while not field.done():\n",
    "    action = np.argmax(q_table[field.state])\n",
    "\n",
    "    (next_x, next_y), _ = field.update_next_state(action)\n",
    "    path[next_x][next_y] = steps\n",
    "    steps +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a3e0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., nan,  1.,  2.,  3.,  4.,  5.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc9e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
